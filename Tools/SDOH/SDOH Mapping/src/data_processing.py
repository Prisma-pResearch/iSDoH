import os
import shutil
import subprocess
import pandas as pd
import logging
from sqlalchemy import create_engine

#Generate latitude and longitude from address infomation
def generate_coordinates_degauss(address_data_df, columns, threshold,save_intermediate=False):
    """
    Preprocess address data, execute a Docker-based geocoding tool, and retrieve geolocation data using Degauss.
    
    Parameters:
    df (pandas.DataFrame): DataFrame containing address data
    columns (list of str): List of column names representing address information
    threshold (float): Threshold for the geocoder's score (accuracy)
    save_intermediate (bool): Whether to save the intermediate preprocessed CSV before geocoding (default is False)
    
    Returns:
    str: Name of the geocoded CSV file generated by the Docker container
    """

    # Convert specified columns to string type and preprocess address
    for col in columns:
        address_data_df[col] = address_data_df[col].astype(str)
    
    # Handle single column or concatenate multiple columns
    if len(columns) == 1:
        # Clean and title case single column address
        address_data_df['address'] = address_data_df[columns[0]].str.title().replace(r'[^a-zA-Z0-9 ]', ' ', regex=True)
    else:
        # Concatenate multiple columns into a single address field
        address_data_df['address'] = address_data_df.apply(lambda row: ' '.join(row[columns]).lower(), axis=1)
        address_data_df['address'] = address_data_df['address'].str.title()
        address_data_df['address'] = address_data_df['address'].replace(r'[^a-zA-Z0-9 ]', ' ', regex=True)

    # Reorder columns to ensure 'address' is the first column
    cols = ['address'] + [col for col in address_data_df.columns if col != 'address']
    address_data_df = address_data_df[cols]
    
    # Drop original address columns if they are no longer needed
    if len(columns) > 1:
        address_data_df.drop(columns=columns, inplace=True)
    
    # Save the preprocessed DataFrame to CSV
    intermediate_csv_path = './address_data_preprocessed.csv'  #address
 
    if save_intermediate:
        address_data_df.to_csv(intermediate_csv_path, index=False)
        logging.info(f"Preprocessed data saved to {intermediate_csv_path}")
    
    # Define the Docker command
    docker_command = [
        'docker', 'run', '--rm',
        '-v', f'"{os.getcwd()}:/tmp"',  # Volume mount for the current directory
        'ghcr.io/degauss-org/geocoder:3.3.0',
        f"/tmp/{os.path.basename(intermediate_csv_path)}"
        str(threshold)
    ]
    
    try:
        # Execute Docker command to run the geocoder
        result = subprocess.run(' '.join(docker_command), shell=True, check=True, capture_output=True, text=True)
        logging.info("Docker command executed successfully.")
        logging.debug(result.stdout)
    except subprocess.CalledProcessError as e:
        logging.error(f"Error executing Docker command: {e}")
        logging.error(e.stderr)
        return None

    # Define the output file name from the geocoder
    output_file_name = f"address_data_geocoder_3.3.0_score_threshold_{threshold}.csv"

    logging.info(f"Geocoded data saved to {output_file_name}")

    return output_file_name

#Generate the FIPS code from latitude and longitude
def generate_fips_degauss(lat_long_data_df, year):
    
    """
    Generates FIPS codes from latitude and longitude using a Docker-based geocoding service.

    This function saves the provided DataFrame to a CSV file, runs a Docker container to process this file
    and generate FIPS codes, and then updates the DataFrame with the FIPS codes before saving the final output.

    Parameters:
    df (pd.DataFrame): The input DataFrame containing latitude and longitude data.
    year (int): The year to be used in the processing and output file naming.

    Returns:
    str or None: The path to the output file if successful, None otherwise.
    """
    logging.info('Generating FIPS...')

    preprocessed_file_path = './lat_long_data_for_processing.csv'
#     df.drop(columns={'matched_street','matched_zip','matched_city', 'matched_state', 'address', 'score', 'precision', 'geocode_result'}, inplace=True)
    lat_long_data_df.to_csv(preprocessed_file_path, index=False)
    
    output_file = f"lat_long_census_block_group_0.6.0_{year}.csv"    

     # Define the Docker command for generating FIPS codes
    docker_command = [
        "docker", "run", "--rm",
        "-v", f"{os.getcwd()}:/tmp",  # Volume mount for the current directory
        "ghcr.io/degauss-org/census_block_group:0.6.0",
        f"/tmp/{os.path.basename(preprocessed_file_path)}",  # Use the file name inside /tmp
        str(year)
    ]
    try:
        result = subprocess.run(docker_command, check=True, capture_output=True, text=True)
        logging.info("Docker command executed successfully.")
        logging.debug(result.stdout)
    except subprocess.CalledProcessError as e:
        logging.error(f"Error executing Docker command: {e}")
        logging.error(e.stderr)
        return None

    if os.path.exists(output_file):
        logging.info(f"Output file generated: {output_file}")
        fips_df = pd.read_csv(output_file)

        # Add FIPS code and clean up unnecessary columns
        fips_df['FIPS'] = fips_df[f'census_tract_id_{year}']
        fips_df.drop(columns=[f'census_block_group_id_{year}', f'census_tract_id_{year}'], inplace=True)
        fips_df.to_csv(output_file, index=False)
        return output_file
    else:
        logging.error(f"Expected output file not found: {output_file}")
        return None

#Link to SDoH database using fips code and date column.This version is conbined all year csv output in one csv file(keep the original datasets column name)
def sdoh_linkage(fips_df, date_column, output_file_path):
    
    """
    Links SDoH data with the provided DataFrame based on FIPS codes and a date column.
    The function fetches relevant data from a PostgreSQL database for each year, merges it with the provided DataFrame,
    and saves the resulting DataFrame to a CSV file.

    Parameters:
    df (pd.DataFrame): Input DataFrame containing FIPS codes and date information.
    date_column (str): The name of the column in `df` that contains date information.
    output_file_path (str): Path to save the final output CSV file.
    """
    fips_df[date_column] = pd.to_datetime(fips_df[date_column])

    # Convert date_column to datetime and extract the year
    fips_df['year'] = fips_df[date_column].dt.year
    fips_df['year'] = fips_df['year'].clip(lower=2012, upper=2023)

    # Ensure FIPS codes are formatted as strings without decimals
    fips_df['FIPS'] = fips_df['FIPS'].apply(lambda x: str(x).split('.')[0])

    # PostgreSQL connection setup
    DATABASE_URL = "postgresql://<username>:<password>@<host>:<port>/<database>"
    engine = create_engine(DATABASE_URL)

    all_data_frames = []
    all_index_data_frames = []  # List to store data from all index tables

    for year, group_df in fips_df.groupby('year'):
        fips_list = ', '.join(f"'{fip}'" for fip in group_df['FIPS'].unique())
        fips_condition = f"geocode IN ({fips_list})"
        
        logging.debug(f"Year: {year}, FIPS condition: {fips_condition}")
        
        #Query to find index tables relevant for the year
        data_source_query = f"""
        SELECT variables_index_name
        FROM data.data_source 
        WHERE EXTRACT(YEAR FROM effective_start_timestamp) <= {year}
          AND EXTRACT(YEAR FROM effective_end_timestamp) >= {year}
          AND boundary_type='Census tract'
          AND geometry_y_n='0';
        """
        logging.debug(f"Data source query: {data_source_query}")

        variables_index_df = pd.read_sql(data_source_query, engine)
        
        if variables_index_df.empty:
            logging.warning(f"No data found in data source for year {year}. Exiting function.")
            continue
        
        # Get all the index information for variables
        for _, v_row in variables_index_df.iterrows():
            index_table_name = v_row['variables_index_name']
            index_table_query = f"""
            SELECT *
            FROM data.\"{index_table_name}\"
            """
            logging.debug(f"Variable table query: {index_table_query}")

            index_table_df = pd.read_sql(index_table_query, engine)
            logging.info(f"Data from {index_table_name} retrieved")

            if index_table_df.empty:
                logging.warning(f"No data returned for {index_table_name}. Skipping...")
                continue

            # Add a column to identify the source table
            index_table_df['source_table'] = index_table_name

            # Append the data to the list of DataFrames
            all_index_data_frames.append(index_table_df)
            logging.info(f"Data from {index_table_name} appended successfully")

        # Process variable tables
        for _, v_row in variables_index_df.iterrows():
            variable_table_name = v_row['variables_index_name'][:-6] + '_variables'
            variable_table_query = f"""
            SELECT *
            FROM data.\"{variable_table_name}\"
            WHERE {fips_condition}
            """
            # logging.debug(f"Variable table query: {variable_table_query}")

            variable_table_df = pd.read_sql(variable_table_query, engine)

            if variable_table_df.empty:
                logging.warning(f"No data returned for {variable_table_name}. Skipping...")
                continue

            # Convert timestamps to datetime
            variable_table_df['effective_start_timestamp'] = pd.to_datetime(variable_table_df['effective_start_timestamp'])
            variable_table_df['effective_end_timestamp'] = pd.to_datetime(variable_table_df['effective_end_timestamp'])

            #Link to SDoH database using FIPS code and date column
            filtered_df = variable_table_df[(variable_table_df['geocode'].isin(group_df['FIPS'])) &
                                            (variable_table_df['effective_start_timestamp'] <= group_df[date_column].max()) &
                                            (variable_table_df['effective_end_timestamp'] >= group_df[date_column].min())]

            if not filtered_df.empty:
                group_df = pd.merge(group_df, filtered_df, left_on='FIPS', right_on='geocode', how='left')
                group_df.drop(columns=['geocode', 'effective_start_timestamp', 'effective_end_timestamp'], inplace=True)

        # Append the processed data for each year
        all_data_frames.append(group_df)
    
    # Combine all the DataFrames from the index tables
    if all_index_data_frames:
        combined_index_df = pd.concat(all_index_data_frames, ignore_index=True, sort=False)
        # Drop duplicate rows
        combined_index_df.drop_duplicates(inplace=True)
        # Ensure directory exists and save the CSV
        os.makedirs('./Linkage_result', exist_ok=True)
        index_file_path = './Linkage_result/Index_data.csv'
        combined_index_df.to_csv(index_file_path, index=False)
        logging.info(f'{index_file_path} written successfully.')


    # Combine all yearly data into one final DataFrame
    final_df = pd.concat(all_data_frames, ignore_index=True, sort=False)
#     output_file = './Linkage_result/SDoH_linkage_Degauss_full.csv'
    
    final_df.rename(columns={
        'lat': 'latitude',
        'lon': 'longitude'}, inplace=True)
    final_df.to_csv(output_file_path, index=False)
    logging.info(f'Output file saved successfully: {output_file_path}')

    #Clean up the database connection
    engine.dispose()
    

#This function run three parts of category   
def process_directory(directory):

    """
    Processes files in the given directory based on their type and performs different actions such as geocoding, FIPS generation,
    and linking to SDoH databases. Outputs results to categorized subdirectories within './Linkage_result'.
    
    Parameters:
    directory (str): The directory containing the files to process.
    """
    # Set base configurations
    output_base = './Linkage_result'
    os.makedirs(output_base, exist_ok=True)
    threshold = 0.7
    columns = ['address_1', 'city', 'state', 'zip']
    date_column = 'visit_start_date'
    default_index_file = './Linkage_result/Index_data.csv'
    
    # Determine the type of processing based on the directory name
    if 'valid_address' in directory:
        process_type = 'address'
    elif 'valid_lat_long' in directory:
        process_type = 'latlong'
    elif 'invalid_lat_lon_address' in directory:
        process_type = 'invalid'
    else:
        logging.error("Unknown directory type. Please check the directory path.")
        return
    
    output_dir = os.path.join(output_base, process_type)
    os.makedirs(output_dir, exist_ok=True)
    logging.info(f"Processing type: {process_type}")

    # Set base configurations
    for idx, filename in enumerate(sorted(os.listdir(directory))):
        filepath = os.path.join(directory, filename)
        logging.info(f"Processing file: {filepath}")

        # Handle 'invalid' files by simply copying them to the new directory
        if process_type == 'invalid':
            final_output = os.path.join(output_dir, f'{process_type}_no_linkage_{idx+1}.csv')
            shutil.copy(filepath, final_output)
            logging.info(f"Invalid file copied to {final_output}")
            
        #Handle 'address' type files: process geocoding, FIPS generation, and linking to SDoH database
        if process_type == 'address':
            final_output = os.path.join(output_dir, f'{process_type}_linkaged_SDoH_{idx+1}.csv')
            # Process starting from geocoding
            df = pd.read_csv(filepath)
                
             # Generate coordinates and process geocoded data
            geocoded_file = generate_coordinates_degauss(df, columns, 0.7,save_intermediate=True)
                
            if geocoded_file:
                df = pd.read_csv(geocoded_file)
                df[date_column] = pd.to_datetime(df[date_column], errors='coerce')
                   # Map year based on date_column and clip to either 2010 or 2020
                df['year'] = df[date_column].dt.year.apply(lambda x: 2010 if x < 2020 else 2020)
                df.drop(columns=['address', 'latitude', 'longitude', 'matched_street', 'matched_zip', 'matched_city', 'matched_state', 'score', 'precision', 'geocode_result'], inplace=True)
                
            # Now, process only for the two distinct mapped years
            for year in [2010, 2020]:
                generate_fips_degauss(df, year)
                
            # Load and process both FIPS files if available
            fips_file_2010 = "lat_long_census_block_group_0.6.0_2010.csv"
            fips_file_2020 = "lat_long_census_block_group_0.6.0_2020.csv"
    
            if os.path.exists(fips_file_2010) and os.path.exists(fips_file_2020):
                fips_df_2010 = pd.read_csv(fips_file_2010)
                fips_df_2020 = pd.read_csv(fips_file_2020)
                    
                fips_df_2010[date_column] = pd.to_datetime(fips_df_2010[date_column], errors='coerce')
                fips_df_2020[date_column] = pd.to_datetime(fips_df_2020[date_column], errors='coerce')
                
                #Keep year < 2020 in fips 2010 version, keep year >= 2020 in fips 2020 version
                fips_df_2010 = fips_df_2010[fips_df_2010[date_column].dt.year < 2020]
                fips_df_2020 = fips_df_2020[fips_df_2020[date_column].dt.year >= 2020]

                logging.info("Rows after filtering 2010:", len(fips_df_2010))
                logging.info("Rows after filtering 2020:", len(fips_df_2020))

                all_fips_df = pd.concat([fips_df_2010, fips_df_2020], ignore_index=True)
                logging.info("Combined rows count:", len(all_fips_df))
                final_output = os.path.join(output_dir, f'Address_linkaged_SDoH_{idx+1}.csv')
                sdoh_linkage(all_fips_df, date_column, final_output)
                new_index_file = os.path.join(output_dir, f'index_data_{os.path.basename(directory)}_{idx+1}.csv')
                if os.path.exists(default_index_file):
                    os.rename(default_index_file, new_index_file)
                    logging.info(f"Index file renamed to {new_index_file}")
            logging.info("Address processing to be implemented.")
                                
        # Call function to genrate fips code and link to SDoH database        
        elif process_type == 'latlong':
            # Process starting from FIPS generation
             final_output = os.path.join(output_dir, f'{process_type}_linkaged_SDoH_{idx+1}.csv')
             df = pd.read_csv(filepath)  
             df.rename(columns={'latitude': 'lat', 'longitude': 'lon'}, inplace=True)
             df[date_column] = pd.to_datetime(df[date_column], errors='coerce')
             # Map year based on date_column and clip to either 2010 or 2020
             df['year'] = df[date_column].dt.year.apply(lambda x: 2010 if x < 2020 else 2020)
             
             for year in [2010, 2020]:
                 fips_file = generate_fips_degauss(df, year)
                    
             # Load and process both FIPS files if available
             fips_file_2010 = "lat_long_census_block_group_0.6.0_2010.csv"
             fips_file_2020 = "lat_long_census_block_group_0.6.0_2020.csv"
    
             if os.path.exists(fips_file_2010) and os.path.exists(fips_file_2020):
                 fips_df_2010 = pd.read_csv(fips_file_2010)
                 fips_df_2020 = pd.read_csv(fips_file_2020)
                    
                 fips_df_2010[date_column] = pd.to_datetime(fips_df_2010[date_column], errors='coerce')
                 fips_df_2020[date_column] = pd.to_datetime(fips_df_2020[date_column], errors='coerce')
                    
                 #Keep year < 2020 in fips 2010 version, keep year >= 2020 in fips 2020 version   
                 fips_df_2010 = fips_df_2010[fips_df_2010[date_column].dt.year < 2020]
                 fips_df_2020 = fips_df_2020[fips_df_2020[date_column].dt.year >= 2020]

                 logging.info("Rows after filtering 2010:", len(fips_df_2010))
                 logging.info("Rows after filtering 2020:", len(fips_df_2020))

                 all_fips_df = pd.concat([fips_df_2010, fips_df_2020], ignore_index=True)
                 logging.info("Combined rows count:", len(all_fips_df))
                 final_output = os.path.join(output_dir, f'Latlong_linkaged_SDoH_{idx+1}.csv')
                 sdoh_linkage(all_fips_df, date_column, final_output)
                 
                 new_index_file = os.path.join(output_dir, f'index_data_{os.path.basename(directory)}_{idx+1}.csv')
                 
                 if os.path.exists(default_index_file):
                     os.rename(default_index_file, new_index_file)
                     logging.info(f"Index file renamed to {new_index_file}")     
        logging.info("Processing complete for directory:", directory) 